import numpy as np
import pandas as pd

from sklearn.base import BaseEstimator
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

## Accuracy might not be right tool to judge model performance
## Example 1

class DummyClassifier(BaseEstimator):
    def fit(self, x, y=None):
        pass # don't train
    
    def predict(self, x):
        pred = np.zeros( (x.shape[0], 1) )
        for i in range(x.shape[0]): # row direction
            if x['Sex'].iloc[i] == 1:
                pred[i] = 0 
            else:
                pred[i] = 1
        return pred
        # if men -> predict as dead, if women -> predict as survived


def fillna(df): # null data
    df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Cabin'].fillna('N', inplace=True)
    df['Embarked'].fillna('N', inplace=True)
    df['Fare'].fillna(0, inplace=True)
    return df

def drop_features(df):
    # when drop unnecessary data, using axis = 1 (drop column)
    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return df 

def encode_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = encode_features(df)
    return df 
    
titanic_df = pd.read_csv('./titanic_train.csv')
y_titanic_df = titanic_df['Survived']
x_titanic_df = titanic_df.drop('Survived', axis=1)
x_titanic_df = transform_features(x_titanic_df)
x_train, x_test, y_train, y_test = train_test_split(x_titanic_df, y_titanic_df, \
                                                   test_size = 0.2, random_state=121)

clf = DummyClassifier()
clf.fit(x_train, y_train)
pred = clf.predict(x_test)
print('Dummy Classifier accuracy: {0:.4f}'.format(accuracy_score(y_test, pred))) # Dummy Classifier accuracy: 0.7542


## Accuracy might not be right tool to judge model performance
## Example 2

from sklearn.datasets import load_digits

class FakeClassifier(BaseEstimator):
    def fit(self, x, y):
        pass
    
    def predict(self, x):
        return np.zeros( (len(x),1), dtype = bool)
    
digits = load_digits() # load mnist data

print(digits.data)
# [[ 0.  0.  5. ...  0.  0.  0.]
# [ 0.  0.  0. ... 10.  0.  0.]
# [ 0.  0.  0. ... 16.  9.  0.]
# ...
# [ 0.  0.  1. ...  6.  0.  0.]
# [ 0.  0.  2. ... 12.  0.  0.]
# [ 0.  0. 10. ... 12.  1.  0.]]
print('## digits data shape: ', digits.data.shape) ## digits data shape:  (1797, 64)
print(digits.target) # [0 1 2 ... 8 9 8]
print('## digits target shape: ', digits.target.shape) ## digits target shape:  (1797,)   

print(digits.target == 7) # array([False, False, False, ..., False, False, False])
y = (digits.target == 7).astype(int) # if 7 -> 1, not 7 -> 0
x_train, x_test, y_train, y_test = train_test_split(digits.data, y, random_state=121)

print('label test set shape: ', y_test.shape) # label test set shape:  (450,)
print('test set label 0 / 1 ') 
print(pd.Series(y_test).value_counts())
# 0    408
# 1     42

fakeclf = FakeClassifier()
fakeclf.fit(x_train, y_train) 
fakepred = fakeclf.predict(x_test)
print('fake prediction will be : {:3f}'.format(accuracy_score(y_test, fakepred))) # fake prediction will be : 0.906667


## Other methods to evaluate model than accuracy
## 1. Confusion Matrix

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, fakepred)
# array([[408,   0],
#       [ 42,   0]], dtype=int64)

## 2. precision and recall
from sklearn.metrics import precision_score, recall_score
print('precision score: ', precision_score(y_test, fakepred)) # precision score:  0.0
print('recall score: ', recall_score(y_test, fakepred)) # recall score:  0.0

def get_evaluation(y_test, pred):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    print('confusion matrix')
    print(confusion)
    print('accuracy: {0:.4f}, precision: {1:.4f}, recall: {2:.4f}'.format(accuracy, precision, recall))

## evaluate titanic model with different methods
from sklearn.linear_model import LogisticRegression

titanic_df = pd.read_csv('./titanic_train.csv')
y_titanic_df = titanic_df['Survived']
x_titanic_df = titanic_df.drop('Survived', axis=1)
x_titanic_df = transform_features(x_titanic_df)

x_train, x_test, y_train, y_test = train_test_split(x_titanic_df, y_titanic_df, \
                                                   test_size=0.2, random_state=121)

lr_clf = LogisticRegression()
lr_clf.fit(x_train, y_train)
pred = lr_clf.predict(x_test)
get_evaluation(y_test, pred)
# confusion matrix
# [[88 19]
#  [20 52]]
# accuracy: 0.7821, precision: 0.7324, recall: 0.7222

pred_proba = lr_clf.predict_proba(x_test)
pred = lr_clf.predict(x_test)
print('pred_proba result shape: {0}'.format(pred_proba.shape)) # Negative, Positive
# pred_proba result shape: (179, 2)
print('pred_proba array sample: \n', pred_proba[:3])
# pred_proba array sample: 
# [[0.85736673 0.14263327]
# [0.85373914 0.14626086]
# [0.79554548 0.20445452]]

pred_proba_result = np.concatenate([pred_proba, pred.reshape(-1, 1)], axis=1)
print('concatenate predicted probability of negative / positive and predicted value', pred_proba_result[:3])
# concatenate predicted probability of negative / positive and predicted value 
# [[0.85736673 0.14263327 0.        ]
# [0.85373914 0.14626086 0.        ]
# [0.79554548 0.20445452 0.        ]]

## Binarizer and custom predict
from sklearn.preprocessing import Binarizer

X = [[1, -1, 2],
    [2, 0, 0],
    [0, 1.1, 1.2]]

binarizer = Binarizer(threshold=1.1)
print(binarizer.fit_transform(X)) 
# if less than threshold -> 0, bigger than threshold -> 1
# [[0. 0. 1.]
# [1. 0. 0.]
# [0. 0. 1.]]

# case 1. custom threshold = 0.5
custom_threshold = 0.5
pred_proba_pos = pred_proba[:,1].reshape(-1, 1)
# use positive column from pred_proba, and make it as 2 dimension 
# Binarizer receive two dimension

binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_pos)
custom_predict = binarizer.transform(pred_proba_pos)
get_evaluation(y_test, custom_predict)
# confusion matrix
# [[88 19]
# [20 52]]
# accuracy: 0.7821, precision: 0.7324, recall: 0.7222

# case 2. custom threshold = 0.4
custom_threshold = 0.4
pred_proba_pos = pred_proba[:,1].reshape(-1, 1)
binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_pos)
# confusion matrix
# [[85 22]
#  [17 55]]
# accuracy: 0.7821, precision: 0.7143, recall: 0.7639

# case 3. check various threshold
thresholds = [0.40, 0.45, 0.50, 0.55, 0.60]

def get_evaluation_per_threshold(y_test, pred_proba_pos, thresholds):
    for custom_threshold in thresholds:
        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_pos)
        custom_predict = binarizer.transform(pred_proba_pos)
        print('thresholds: ', custom_threshold)
        get_evaluation(y_test, custom_predict)

get_evaluation_per_threshold(y_test, pred_proba[:,1].reshape(-1, 1), thresholds)
custom_predict = binarizer.transform(pred_proba_pos)
get_evaluation(y_test, custom_predict)

# thresholds:  0.4
# confusion matrix
# [[85 22]
#  [17 55]]
# accuracy: 0.7821, precision: 0.7143, recall: 0.7639
# thresholds:  0.45
# confusion matrix
# [[86 21]
#  [18 54]]
# accuracy: 0.7821, precision: 0.7200, recall: 0.7500
# thresholds:  0.5
# confusion matrix
# [[88 19]
#  [20 52]]
# accuracy: 0.7821, precision: 0.7324, recall: 0.7222
# thresholds:  0.55
# confusion matrix
# [[91 16]
#  [22 50]]
# accuracy: 0.7877, precision: 0.7576, recall: 0.6944
# thresholds:  0.6
# confusion matrix
# [[93 14]
#  [23 49]]
# accuracy: 0.7933, precision: 0.7778, recall: 0.6806

## 3. precision recall curve
from sklearn.metrics import precision_recall_curve
pred_proba_pos = lr_clf.predict_proba(x_test)[:, 1]

precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_pos)
print('thresholds shape: ', thresholds.shape) # thresholds shape:  (164,)
print('precisions shape: ', precisions.shape) # precisions shape:  (165,)
print('recalls shape: ', recalls.shape) # recalls shape:  (165,)

print('thresholds samples: ', thresholds[:5]) # thresholds samples:  [0.06572773 0.06847938 0.0688252  0.06898417 0.07128771]
print('precisions samples: ', precisions[:5]) # precisions samples:  [0.41860465 0.41520468 0.41764706 0.42011834 0.42261905]
print('recalls samples: ', recalls[:5]) # recalls samples:  [1.         0.98611111 0.98611111 0.98611111 0.98611111]

thr_index = np.arange(0, thresholds.shape[0], 15)
print('index for sample extraction: ', thr_index) # index for sample extraction:  [  0  15  30  45  60  75  90 105 120 135 150]
print('thresholds for samples: ', np.round(thresholds[thr_index], 2)) # thresholds for samples:  [0.07 0.11 0.12 0.15 0.2  0.28 0.44 0.62 0.68 0.77 0.91]
print('precisions for samples: ', np.round(precisions[thr_index], 3)) # precisions for samples:  [0.419 0.452 0.496 0.525 0.579 0.648 0.711 0.787 0.886 1.    1.   ]
print('recalls for samples: ', np.round(recalls[thr_index], 3)) # recalls for samples:  [1.    0.986 0.958 0.889 0.861 0.819 0.75  0.667 0.542 0.403 0.194]

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
%matplotlib inline

# draw precision recall curve
def precision_recall_curve_plot(y_test, pred_proba_pos):
    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_pos)
    plt.figure(figsize=(8,6))
    threshold_boundary = thresholds.shape[0]
    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')
    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')
    
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1), 2))
    plt.xlabel('Threshold value')
    plt.ylabel('Precision and Recall Value')
    plt.legend()
    plt.grid()
    plt.show()

precision_recall_curve_plot(y_test, lr_clf.predict_proba(x_test)[:, 1])

## 4. F1 score
from sklearn.metrics import f1_score
f1 = f1_score(y_test, pred)
print('f1 score: {:.4f}'.format(f1)) # f1 score: 0.7273

def get_evaluation(y_test, pred):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    print('confusion matrix')
    print(confusion)
    print('accuracy: {0:.4f}, precision: {1:.4f}, recall: {2:.4f}, F1: {3:.4f}'.format(accuracy, precision, recall, f1))

thresholds = [0.40, 0.45, 0.50, 0.55, 0.60]
pred_proba = lr_clf.predict_proba(x_test) # prediction probability of negative, positive
get_evaluation_per_threshold(y_test, pred_proba[:,1].reshape(-1, 1), thresholds)
# thresholds:  0.4
# confusion matrix
# [[85 22]
#  [17 55]]
# accuracy: 0.7821, precision: 0.7143, recall: 0.7639, F1: 0.7383
# thresholds:  0.45
# confusion matrix
# [[86 21]
#  [18 54]]
# accuracy: 0.7821, precision: 0.7200, recall: 0.7500, F1: 0.7347
# thresholds:  0.5
# confusion matrix
# [[88 19]
#  [20 52]]
# accuracy: 0.7821, precision: 0.7324, recall: 0.7222, F1: 0.7273
# thresholds:  0.55
# confusion matrix
# [[91 16]
#  [22 50]]
# accuracy: 0.7877, precision: 0.7576, recall: 0.6944, F1: 0.7246
# thresholds:  0.6
# confusion matrix
# [[93 14]
#  [23 49]]
# accuracy: 0.7933, precision: 0.7778, recall: 0.6806, F1: 0.7259

## 5. ROC curve and AUC
from sklearn.metrics import roc_curve

pred_proba_pos = lr_clf.predict_proba(x_test)[:, 1]
fprs, tprs, thresholds = roc_curve(y_test, pred_proba_pos)
thr_index = np.arange(0, thresholds.shape[0], 5)
print('index array for samples: ', thr_index) # index array for samples:  [ 0  5 10 15 20 25 30 35 40 45 50 55 60]
print('thresholds for samples: ', np.round(thresholds[thr_index], 2)) # thresholds for samples:  [1.97 0.75 0.68 0.63 0.59 0.53 0.37 0.28 0.21 0.17 0.14 0.12 0.11]
print('fpr per sample: ', np.round(fprs[thr_index], 3)) # fpr per sample:  [0.    0.019 0.047 0.103 0.14  0.178 0.224 0.299 0.374 0.458 0.607 0.673 0.804]
print('tpr per sample: ', np.round(tprs[thr_index], 3)) # tpr per sample:  [0.    0.486 0.542 0.583 0.681 0.722 0.778 0.833 0.861 0.889 0.917 0.958 0.986]

# draw tpr(recall) and fpr(1-sensitivity) curve
def roc_curve_plot(y_test, pred_proba_pos):
    fprs, tprs, thresholds = roc_curve(y_test, pred_proba_pos)
    plt.plot(fprs, tprs, label='ROC')
    plt.plot([0, 1], [0, 1], 'k--', label='Random') # diagonal
    
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1), 2))
    plt.xlim(0,1)
    plt.ylim(0,1)
    plt.xlabel('FPR( 1 - Sensitivity )')
    plt.ylabel('TPR( Recall )')
    plt.legend()
    plt.show()

from sklearn.metrics import roc_auc_score

pred_proba = lr_clf.predict_proba(x_test)[:, 1]
roc_score = roc_auc_score(y_test, pred_proba)
print('ROC AUC value: {:.4f}'.format(roc_score))
roc_curve_plot(y_test, lr_clf.predict_proba(x_test)[:, 1]) 

from sklearn.metrics import roc_auc_score

pred_proba = lr_clf.predict_proba(x_test)[:, 1]
roc_score = roc_auc_score(y_test, pred_proba)
print('ROC AUC value: {:.4f}'.format(roc_score)) # ROC AUC value: 0.8529

def get_evaluation(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('confusion matrix')
    print(confusion)
    print('accuracy: {0:.4f}, precision: {1:.4f}, recall: {2:.4f}, F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
